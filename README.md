# Numerical Optimization Algorithms from Scratch

## Overview
This repository contains implementations of various numerical optimization algorithms coded from scratch. These algorithms are fundamental to machine learning and deep learning optimization processes, ensuring efficient parameter updates and convergence.

## Implemented Algorithms
The repository includes the following optimization techniques:

### 1. **Gradient Descent Variants**
- **Batch, Mini-Batch, and Stochastic Gradient Descent** ([Notebook](Batch-MiniBatch-Stochastic.ipynb))
- **Basic Gradient Descent for Linear Regression (Single & Multi-variable)** ([Notebook](GD_Implementation_for_LR_Single_and_MultiVar.ipynb))

### 2. **Adaptive Learning Rate Methods**
- **Adagrad, RMSProp, and Adam** ([Notebook](Adagrad, RMSProp, and Adam.zip))

### 3. **Momentum-based Optimization**
- **Momentum and Nesterov Accelerated Gradient (NAG)** ([Notebook](Momentum and NAG.ipynb))

### 4. **Second-Order Optimization Methods**
- **Newton's Method and BFGS (Quasi-Newton Method)** ([Notebook](Newton-BFGS.ipynb))

## Features
- Implemented from scratch without relying on high-level optimization libraries.
- Demonstrations using simple datasets.
- Comparison of different optimization techniques.
- Performance visualization through plots.

## Getting Started
### Prerequisites
To run these notebooks, ensure you have the following installed:
- Python (>=3.7)
- Jupyter Notebook
- NumPy
- Matplotlib



